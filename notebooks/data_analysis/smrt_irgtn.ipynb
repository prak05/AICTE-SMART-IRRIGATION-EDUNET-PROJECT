{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNWQZ42ayL6T"
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "# Chalo bhai, sab zaroori libraries import kar lete hain. Koi bhoolna mat, warna 'ModuleNotFoundError' ka error aayega aur poori raat debugging karni padegi! 🤪\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_file_cell"
   },
   "outputs": [],
   "source": [
    "# Ab apni CSV file upload karte hain. Ye step mat bhoolna, warna 'FileNotFoundError' aayega aur sir dukh jayega! 😅\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZphx0alyQBJ"
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 1: DATA LOAD AUR PREPROCESSING\n",
    "# -------------------------------\n",
    "\n",
    "# Dataset ko load karte hain. Ek baar check kar lena file name sahi hai ya nahi. Spelling mistake se project latak jata hai. 😉\n",
    "df = pd.read_csv(\"irrigation_machine.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oMJszLhycGN"
   },
   "outputs": [],
   "source": [
    "# Pehle 5 rows dekhte hain. Jisse data ki shakal pata chal jaye. 'df.tail()' se last 5 rows bhi dekh sakte hain, just for fun. 😄\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTSbYHQwyi4z"
   },
   "outputs": [],
   "source": [
    "# Ab data ki kundli nikalte hain. Pura info, jaise kitne columns, koi missing values toh nahi... agar missing hai toh agle step mein dhyan rakhna padega. 🧐\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3IYL_L9zKyS"
   },
   "outputs": [],
   "source": [
    "# 'Unnamed: 0' column ek फालतू column lag raha hai. Ise nikal dete hain. Bheed kam, kaam zyada. 💪\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() # Data ka stats, jaise min, max, average. Isse outliers bhi pakad sakte hain. 🕵️‍♂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4CIhf400Nvc"
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 2: FEATURES AUR LABELS\n",
    "# -------------------------------\n",
    "\n",
    "# Input (X) aur Output (y) ko alag-alag karte hain. X hamare sensors hain, aur y hamari sprinkler's. Simple, right? 🧠\n",
    "X = df.iloc[:, 0:20]   # sensor_0 to sensor_19\n",
    "\n",
    "y = df.iloc[:, 20:]    # parcel_0, parcel_1, parcel_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNxVvh3t02td"
   },
   "outputs": [],
   "source": [
    "X.sample(10) # 10 random samples of X, bas verify karne ke liye ki sab theek hai. 👌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSsSc--x062S"
   },
   "outputs": [],
   "source": [
    "y.sample(10) # Aur yahan 10 random samples of y. Check karlo ki 0 aur 1 hi hai na. Binary classification hai bhai! 🤖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info() # X ka info. Ab saare columns float64 hone chahiye. Jo nahi hain, unko sahi karo! 😠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.info() # y ka info. Saare columns int64 hain, perfect! Koi galti nahi. ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X # Poora X print kar rahe hain. Screen bhar jayegi, lekin satisfaction milega. 😌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSaWzYbX1BKG"
   },
   "outputs": [],
   "source": [
    "X.shape, y.shape # Shapes dekh rahe hain. Dono mein rows same honi chahiye. Warna 'ValueError' aayega. 😟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "# Ab data ko scale kar rahe hain. Jisse saare features 0 se 1 ke beech mein aa jayein. Warna model partiality karega. 😒\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asbygD9m1HVD"
   },
   "source": [
    "# -------------------------------\n",
    "# STEP 3: TRAIN-TEST SPLIT\n",
    "# -------------------------------\n",
    "\n",
    "# Ab data ko training aur testing ke liye split karte hain. 80% training, 20% testing. Random_state se har baar same split milega. 🤝\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_B9ZsFu3dIP"
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape # Fir se shape check! Dil ko tasalli milti hai. ❤️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiV9Qy2N1MSx"
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 4: MODEL TRAINING\n",
    "# -------------------------------\n",
    "\n",
    "# Apne model ko banate hain. RandomForestClassifier is powerful, but MultiOutputClassifier is the real boss here. 😎\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# Humne custom hyperparameters set kiye hain. Isse model ki performance aur badhegi.\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,         # Kitne trees honge? Zyada trees, zyada accuracy, lekin training time bhi zyada. 🌲\n",
    "    max_depth=10,             # Har tree kitna deep jayega? Isse overfitting avoid hota hai. 🌳\n",
    "    min_samples_split=4,      # Node ko split karne ke liye kitne samples chahiye? Important hai! 🧐\n",
    "    min_samples_leaf=2,       # Har leaf mein kitne samples honge? Overfitting se bachata hai. 🍃\n",
    "    max_features='sqrt',      # Har split pe kitne features dekhega? 'sqrt' generally accha kaam karta hai. ✨\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# MultiOutputClassifier wrap karte hain, kyuki hume ek sath 3 cheeze predict karni hai. 🤝\n",
    "model = MultiOutputClassifier(rf)\n",
    "\n",
    "# Model ko training data pe fit karte hain. Ab shuru hota hai asli khel. 🚀\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77QRUC-I1P2p"
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 5: MODEL EVALUATION\n",
    "# -------------------------------\n",
    "\n",
    "# Model ne testing data pe kya predict kiya? Chalo dekhte hain. 🤞\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=y.columns)) # Report card aa gaya! Precision, recall, F1-score check karo. 📊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXQ-xuE44CGe"
   },
   "outputs": [],
   "source": [
    "print(df[['parcel_0', 'parcel_1', 'parcel_2']].sum()) # Original data mein total kitni baar sprinklers ON the? Tally kar lo. 🧐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ab data ko plot karte hain. Charts se sab kuch clear ho jata hai. 📈\n",
    "conditions = {\n",
    "    \"Parcel 0 ON\": df['parcel_0'],\n",
    "    \"Parcel 1 ON\": df['parcel_1'],\n",
    "    \"Parcel 2 ON\": df['parcel_2'],\n",
    "    \"Parcel 0 & 1 ON\": df['parcel_0'] & df['parcel_1'],\n",
    "    \"Parcel 0 & 2 ON\": df['parcel_0'] & df['parcel_2'],\n",
    "    \"Parcel 1 & 2 ON\": df['parcel_1'] & df['parcel_2'],\n",
    "    \"All Parcels ON\": df['parcel_0'] & df['parcel_1'] & df['parcel_2'],\n",
    "}\n",
    "\n",
    "# Vertically stacked subplots banate hain. Har condition ke liye alag plot. 🖼️\n",
    "fig, axs = plt.subplots(nrows=len(conditions), figsize=(10,15), sharex=True)\n",
    "\n",
    "# Loop chala ke har condition ka plot banate hain. Ye magic hai! ✨\n",
    "for ax, (title, condition) in zip(axs, conditions.items()):\n",
    "    ax.step(df.index, condition.astype(int), where='post', linewidth=1, color='teal')\n",
    "    ax.set_title(f\"Sprinkler - {title}\")\n",
    "    ax.set_ylabel(\"Status\")\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_yticklabels(['OFF', 'ON'])\n",
    "   \n",
    "\n",
    "# Last subplot mein x-axis label. Sahi plotting ka rule hai ye. 📌\n",
    "axs[-1].set_xlabel(\"Time Index (Row Number)\")\n",
    "\n",
    "# Plot dikha do! 👀\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFshE5RyBsjG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ab ek aur plot. Individual pumps aur combined coverage. Ek hi frame mein saari kahani. 🎬\n",
    "any_pump_on = (df['parcel_0'] == 1) | (df['parcel_1'] == 1) | (df['parcel_2'] == 1)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Har pump ka status plot karte hain. Blue, Orange, Green - sab clear hai. 🎨\n",
    "plt.step(df.index, df['parcel_0'], where='post', linewidth=2, label='Parcel 0 Pump', color='blue')\n",
    "plt.step(df.index, df['parcel_1'], where='post', linewidth=2, label='Parcel 1 Pump', color='orange')\n",
    "plt.step(df.index, df['parcel_2'], where='post', linewidth=2, label='Parcel 2 Pump', color='green')\n",
    "\n",
    "plt.title(\"Pump Activity and Combined Farm Coverage\")\n",
    "plt.xlabel(\"Time Index (Row Number)\")\n",
    "plt.ylabel(\"Status\")\n",
    "plt.yticks([0, 1], ['OFF', 'ON'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sabse important step: model ko save kar rahe hain. Ye hamare project ka final output hai. 📦\n",
    "joblib.dump(model, \"Farm_Irrigation_System.pkl\")\n",
    "print(\"Model saved as 'Farm_Irrigation_System.pkl'. Ab isse Streamlit app mein use karenge. 🎉\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
